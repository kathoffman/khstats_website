---
title: "Become a Superlearner! An Illustrated Guide to Superlearning"
author: "Katherine Hoffman"
date: 2020-10-10T21:13:14-05:00
categories: ["R","statistics"]
draft: false
math: true
tags: ["R","SuperLearner","superlearning","stacking","stacked generalizations","ensemble learning","sl3","rstats"]
output: 
  html_document:
    toc: true
    toc_float: true
    smart: false
    print_df: paged
---



<blockquote>
<p>Why use <em>one</em> machine learning algorithm when you could use all of them?! This post contains a step-by-step walkthrough of how to build a superlearner prediction algorithm in <code>R</code>.</p>
</blockquote>
<html>
<head>
<title>
HTML Image as link
</title>
</head>
<body>
<img alt="cheatsheet" src="/img/Superlearning.jpg"  
         width=100%">
<figcaption>
<strong><em>A Visual Guide…</em></strong> Over the winter, I read <a href="https://www.springer.com/gp/book/9781441997814"><em>Targeted Learning</em></a> by Mark van der Laan and Sherri Rose. This “visual guide” I made for <em>Chapter 3: Superlearning</em> by Rose, van der Laan, and Eric Polley is a condensed version of the following tutorial. It is available as an <a href="https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/Superlearner.pdf">8.5x11" pdf on Github</a>, should you wish to print it out for reference (or desk decor).
</figcaption>
</a>
</body>
</html>
<div id="supercuts-of-superlearning" class="section level1">
<h1>Supercuts of superlearning</h1>
<ul>
<li><p><strong>Superlearning</strong> is a technique for prediction that involves <strong>combining many individual statistical algorithms</strong> (commonly called “data-adaptive” or “machine learning” algorithms) to <strong>create a new, single prediction algorithm</strong> that is expected to <strong>perform at least as well as any of the individual algorithms</strong>.</p></li>
<li><p>The superlearner algorithm “decides” how to combine, or weight, the individual algorithms based upon how well each one <strong>minimizes a specified loss function</strong>, for example, the mean squared error (MSE). This is done using cross-validation to avoid overfitting.</p></li>
<li><p>The motivation for this type of “ensembling” is that <strong>a mix of multiple algorithms may be more optimal for a given data set than any single algorithm</strong>. For example, a tree based model averaged with a linear model (e.g. random forests and LASSO) could smooth some of the model’s edges to improve predictive performance.</p></li>
<li><p>Superlearning is also called stacking, stacked generalizations, and weighted ensembling by different specializations within the realms of statistics and data science.</p></li>
</ul>
<p><img src="/img/spiderman_meme.jpg" style="width:42.0%" /></p>
</div>
<div id="superlearning-step-by-step" class="section level1">
<h1>Superlearning, step by step</h1>
<p>First I’ll go through the algorithm one step at a time using a simulated data set.</p>
<div id="initial-set-up-load-libraries-set-seed-simulate-data" class="section level2">
<h2>Initial set-up: Load libraries, set seed, simulate data</h2>
<p>For simplicity I’ll show the concept of superlearning using only four variables (AKA features or predictors) to predict a continuous outcome. Let’s first simulate a continuous outcome, <code>y</code>, and four potential predictors, <code>x1</code>, <code>x2</code>, <code>x3</code>, and <code>x4</code>.</p>
<pre class="r"><code>library(tidyverse)
library(knitr)
set.seed(7)</code></pre>
<pre class="r"><code>n &lt;- 5000
obs &lt;- tibble(
  id = 1:n,
  x1 = rnorm(n),
  x2 = rbinom(n, 1, plogis(10*x1)),
  x3 = rbinom(n, 1, plogis(x1*x2 + .5*x2)),
  x4 = rnorm(n, mean=x1*x2, sd=.5*x3),
  y = x1 + x2 + x2*x3 + sin(x4)
)
kable(head(obs), digits=3, caption = &quot;Simulated data set&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>Simulated data set</caption>
<thead>
<tr class="header">
<th align="right">id</th>
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">x3</th>
<th align="right">x4</th>
<th align="right">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">2.287</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1.385</td>
<td align="right">5.270</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">-1.197</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.000</td>
<td align="right">-1.197</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">-0.694</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.000</td>
<td align="right">-0.694</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">-0.412</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-0.541</td>
<td align="right">-0.928</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">-0.971</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.000</td>
<td align="right">-0.971</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">-0.947</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-0.160</td>
<td align="right">-1.107</td>
</tr>
</tbody>
</table>
<html>
<body>
<h2 style="color:#c30a0a" >
<strong>Step 1: Split data into K folds
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step1.png" style="width:50.0%" />
The superlearner algorithm relies on K-fold cross-validation (CV) to avoid overfitting. We will start this process by splitting the data into 10 folds. The easiest way to do this is by creating indices for each CV fold.</p>
<pre class="r"><code>k &lt;- 10 # 10 fold cv
cv_index &lt;- sample(rep(1:k, each = n/k)) # create indices for each CV fold. We need each fold K to contain n (all the rows of our data set) divided by k rows. in our example this is 5000/10 = 500 rows in each fold</code></pre>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 2: Fit base learners for first CV-fold
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step2.png" style="width:50.0%" /></p>
<p>Recall that in K-fold CV, each fold serves as the validation set one time. In this first round of CV, we will train all of our base learners on all the CV folds (k = 1,2,…,9) <em>except</em> for the very last one: <code>cv_index == 10</code>.</p>
<p>The individual algorithms or <strong>base learners</strong> that we’ll use here are three linear regressions with differently specified parameters:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Learner A</strong>: <span class="math inline">\(Y=\beta_0 + \beta_1 X_2 + \beta_2 X_4 + \epsilon\)</span></p></li>
<li><p><strong>Learner B</strong>: <span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_3 + \beta_4 sin(X_4) + \epsilon\)</span></p></li>
<li><p><strong>Learner C</strong>: <span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1 X_2 + \beta_5 X_1 X_3 + \beta_6 X_2 X_3 + \beta_7 X_1 X_2 X_3 + \epsilon\)</span></p></li>
</ol>
<pre class="r"><code>cv_train_1 &lt;- obs[-which(cv_index == 10),] # make a data set that contains all observations except those in k=1
fit_1a &lt;- glm(y ~ x2 + x4, data=cv_train_1) # fit the first linear regression on that training data
fit_1b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train_1) # second LR fit on the training data
fit_1c &lt;- glm(y ~ x1*x2*x3, data=cv_train_1) # and the third LR</code></pre>
<p>I am <em>only</em> using the linear regressions so that code for running more complicated regressions does not take away from understanding the general superlearning algorithm.</p>
<p>Superlearning actually works best if you use a diverse set, or <strong>superlearner library</strong>, of base learners. For example, instead of three linear regressions, we could use a least absolute shrinkage estimator (LASSO), random forest, and multivariate adaptive splines (MARS). Any parametric or non-parametric supervised machine learning algorithm can be included as a base learner.</p>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 3: Obtain predictions for first CV-fold
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step3.png" style="width:50.0%" /></p>
<p>We can then get use our validation data, <code>cv_index == 10</code>, to obtain our first set of cross-validated predictions.</p>
<pre class="r"><code>cv_valid_1 &lt;- obs[which(cv_index == 10),] # make a data set that only contains observations except in k=10
pred_1a &lt;- predict(fit_1a, newdata = cv_valid_1) # use that data set as the validation for all the models in the SL library
pred_1b &lt;- predict(fit_1b, newdata = cv_valid_1) 
pred_1c &lt;- predict(fit_1c, newdata = cv_valid_1)</code></pre>
<p>Since we have 5000 <code>obs</code>ervations, that gives us three vectors of length 500: a set of predictions for each of our Learners A, B, and C.</p>
<pre class="r"><code>length(pred_1a) # double check we only have n/k predictions ...we do :-)</code></pre>
<pre><code>## [1] 500</code></pre>
<pre class="r"><code>knitr::kable(head(cbind(pred_1a, pred_1b, pred_1c)), digits= 2, caption = &quot;First CV round of predictions&quot;) </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-6">Table 2: </span>First CV round of predictions</caption>
<thead>
<tr class="header">
<th align="right">pred_1a</th>
<th align="right">pred_1b</th>
<th align="right">pred_1c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-1.39</td>
<td align="right">-0.77</td>
<td align="right">-0.40</td>
</tr>
<tr class="even">
<td align="right">-1.27</td>
<td align="right">-0.34</td>
<td align="right">-0.11</td>
</tr>
<tr class="odd">
<td align="right">2.16</td>
<td align="right">1.32</td>
<td align="right">1.10</td>
</tr>
<tr class="even">
<td align="right">4.27</td>
<td align="right">4.26</td>
<td align="right">3.98</td>
</tr>
<tr class="odd">
<td align="right">3.31</td>
<td align="right">3.98</td>
<td align="right">3.78</td>
</tr>
<tr class="even">
<td align="right">2.29</td>
<td align="right">2.42</td>
<td align="right">2.83</td>
</tr>
</tbody>
</table>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 4: Obtain CV predictions for entire data set
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step4.png" style="width:32.0%" /></p>
<p>We’ll want to get those predictions for <em>every</em> fold. So, using your favorite <code>for</code> loop, <code>apply</code> statement, or <code>map</code>ping function, fit the base learners and obtain predictions for each of them, so that there are 1000 predictions – one for every point in <code>obs</code>ervations.</p>
<p>The way I chose to code this was to make a generic function that combines Step 2 (base learners fit to the training data) and Step 3 (predictions on the validation data), then use <code>map_dfr()</code> from the <code>purrr</code> package to repeat over all 10 CV folds. I saved the results in a new data frame called <code>cv_preds</code>.</p>
<pre class="r"><code>cv_folds &lt;- as.list(1:k)
names(cv_folds) &lt;- paste0(&quot;fold&quot;,1:k)

get_preds &lt;- function(fold){   # function that does the same procedure as step 2 and 3 for any CV fold
  cv_train &lt;- obs[-which(cv_index == fold),]  # make a training data set that contains all data except fold k
  fit_a &lt;- glm(y ~ x2 + x4, data=cv_train)  # fit all the base learners to that data
  fit_b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train)
  fit_c &lt;- glm(y ~ x1*x2*x3, data=cv_train)
  cv_valid &lt;- obs[which(cv_index == fold),]  # make a validation data set that only contains data from fold k
  pred_a &lt;- predict(fit_a, newdata = cv_valid)  # obtain predictions from all the base learners for that validation data
  pred_b &lt;- predict(fit_b, newdata = cv_valid)
  pred_c &lt;- predict(fit_c, newdata = cv_valid)
  return(data.frame(&quot;obs_id&quot; = cv_valid$id, &quot;cv_fold&quot; = fold, pred_a, pred_b, pred_c))  # save the predictions and the ids of the observations in a data frame
}

cv_preds &lt;- purrr::map_dfr(cv_folds, ~get_preds(fold = .x)) # map_dfr loops through every fold (1:k) and binds the rows of the listed results together

cv_preds %&gt;% arrange(obs_id) %&gt;% head() %&gt;% kable(digits=2, caption = &quot;All CV predictions for all three base learners&quot;) </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 3: </span>All CV predictions for all three base learners</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">obs_id</th>
<th align="right">cv_fold</th>
<th align="right">pred_a</th>
<th align="right">pred_b</th>
<th align="right">pred_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1…1</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">3.73</td>
<td align="right">5.42</td>
<td align="right">5.28</td>
</tr>
<tr class="even">
<td>1…2</td>
<td align="right">2</td>
<td align="right">8</td>
<td align="right">-0.77</td>
<td align="right">-1.19</td>
<td align="right">-1.20</td>
</tr>
<tr class="odd">
<td>1…3</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">-0.78</td>
<td align="right">-0.81</td>
<td align="right">-0.69</td>
</tr>
<tr class="even">
<td>1…4</td>
<td align="right">4</td>
<td align="right">10</td>
<td align="right">-1.39</td>
<td align="right">-0.77</td>
<td align="right">-0.40</td>
</tr>
<tr class="odd">
<td>1…5</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">-0.78</td>
<td align="right">-1.01</td>
<td align="right">-0.97</td>
</tr>
<tr class="even">
<td>1…6</td>
<td align="right">6</td>
<td align="right">7</td>
<td align="right">-0.96</td>
<td align="right">-1.04</td>
<td align="right">-0.94</td>
</tr>
</tbody>
</table>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 5: Choose and compute loss function of interest via metalearner
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step5.png" style="width:60.0%" /></p>
<blockquote>
<p>This is the key step of the superlearner algorithm: we will use a new learner, a <strong>metalearner</strong>, to take information from all of the base learners and create that new algorithm.</p>
</blockquote>
<p>Now that we have cross-validated predictions for every observation in the data set, we want to merge those CV predictions back into our main data set…</p>
<pre class="r"><code>obs_preds &lt;- 
  full_join(obs, cv_preds, by=c(&quot;id&quot; = &quot;obs_id&quot;))</code></pre>
<p>…so that we can minimize a final loss function of interest between the true outcome and each CV prediction. This is how we’re going to optimize our overall prediction algorithm: we want to make sure we’re “losing the least” in the way we combine our base learners’ predictions to ultimately make final predictions. We can do this efficiently by choosing a new learner, a metalearner, which reflects the final loss function of interest.</p>
<p>For simplicity, we’ll use another linear regression as our metalearner. Using a linear regression as a metalearner will minimize the Cross-Validated Mean Squared Error (CV-MSE) when combining the base learner predictions. Note that we could use a variety of parametric or non-parametric regressions to minimize the CV-MSE.</p>
<p>No matter what metalearner we choose, the predictors will always be the cross-validated predictions from each base learner, and the outcome will always be the true outcome, <code>y</code>.</p>
<pre class="r"><code>sl_fit &lt;- glm(y ~ pred_a + pred_b + pred_c, data = obs_preds)
kable(broom::tidy(sl_fit), digits=3, caption = &quot;Metalearner regression coefficients&quot;) </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 4: </span>Metalearner regression coefficients</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.003</td>
<td align="right">0.002</td>
<td align="right">-1.447</td>
<td align="right">0.148</td>
</tr>
<tr class="even">
<td align="left">pred_a</td>
<td align="right">-0.017</td>
<td align="right">0.004</td>
<td align="right">-4.739</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">pred_b</td>
<td align="right">0.854</td>
<td align="right">0.007</td>
<td align="right">128.241</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">pred_c</td>
<td align="right">0.165</td>
<td align="right">0.005</td>
<td align="right">30.103</td>
<td align="right">0.000</td>
</tr>
</tbody>
</table>
<p>This metalearner provides us with the coefficients, or weights, to apply to each of the base learners. In other words, if we have a set of predictions from Learner A, B, and C, we can obtain our best possible predictions by starting with an intercept of -0.003, then adding -0.017 <span class="math inline">\(\times\)</span> predictions from Learner A, 0.854 <span class="math inline">\(\times\)</span> predictions from Learner B, and 0.165 <span class="math inline">\(\times\)</span> predictions from Learner C.</p>
<p><em>For more information on the metalearning step, check out the <a href="#appendix">Appendix</a>.</em></p>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 6: Fit base learners on entire data set
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step6.png" style="width:50.0%" /></p>
<p>After we fit the metalearner, we officially have our superlearner algorithm, so it’s time to input data and obtain predictions! To implement the algorithm and obtain final predictions, we first need to fit the base learners on the full data set.</p>
<pre class="r"><code>fit_a &lt;- glm(y ~ x2 + x4, data=obs)
fit_b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs)
fit_c &lt;- glm(y ~ x1*x2*x3, data=obs)</code></pre>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 7: Obtain predictions from each base learner on entire data set
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step7.png" style="width:40.0%" /></p>
<p>We’ll use <em>those</em> base learner fits to get predictions from each of the base learners for the entire data set, and then we will plug those predictions into the metalearner fit. Remember, we were previously using cross-validated predictions, rather than fitting the base learners on the whole data set. This was to avoid overfitting.</p>
<pre class="r"><code>pred_a &lt;- predict(fit_a)
pred_b &lt;- predict(fit_b)
pred_c &lt;- predict(fit_c)
full_data_preds &lt;- tibble(pred_a, pred_b, pred_c)</code></pre>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 8: Use metalearner fit to weight base learners
</h1>
</strong>
</body>
</html>
<p><img src="/img/sl_steps/step8.png" style="width:60.0%" /></p>
<p>Once we have the predictions from the full data set, we can input them to the metalearner, where the output will be a final prediction for <code>y</code>.</p>
<pre class="r"><code>sl_predictions &lt;- predict(sl_fit, newdata = full_data_preds)
kable(head(sl_predictions), col.names = &quot;sl_predictions&quot;, digits= 2, caption = &quot;Final SL predictions (manual)&quot;) </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-12">Table 5: </span>Final SL predictions (manual)</caption>
<thead>
<tr class="header">
<th align="right">sl_predictions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.44</td>
</tr>
<tr class="even">
<td align="right">-1.20</td>
</tr>
<tr class="odd">
<td align="right">-0.79</td>
</tr>
<tr class="even">
<td align="right">-0.71</td>
</tr>
<tr class="odd">
<td align="right">-1.02</td>
</tr>
<tr class="even">
<td align="right">-1.03</td>
</tr>
</tbody>
</table>
<p>And… that’s it! Those are our superlearner predictions for the full data set.</p>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 9: Obtain predictions on new data
</h1>
</strong>
</body>
</html>
<p>We can now modify Step 7 and Step 8 to accommodate any new observation(s):</p>
<blockquote>
<p><strong>To predict on new data:</strong><br> 1. Use the fits from each base learner to obtain base learner predictions for the new observation(s).<br> 2. Plug those base learner predictions into the metalearner fit.</p>
</blockquote>
<p>We can generate a single <code>new_obs</code>ervation to see how this would work in practice.</p>
<pre class="r"><code>new_obs &lt;- tibble(x1 = .5, x2 = 0, x3 = 0, x4 = -3)
new_pred_a &lt;- predict(fit_a, new_obs)
new_pred_b &lt;- predict(fit_b, new_obs)
new_pred_c &lt;- predict(fit_c, new_obs)
new_pred_df &lt;- tibble(&quot;pred_a&quot; = new_pred_a, &quot;pred_b&quot; = new_pred_b, &quot;pred_c&quot; = new_pred_c)
predict(sl_fit, newdata = new_pred_df)</code></pre>
<pre><code>##         1 
## 0.1181052</code></pre>
<p>Our superlearner model predicts that an observation with predictors <span class="math inline">\(X_1=.5\)</span>, <span class="math inline">\(X_2=0\)</span>, <span class="math inline">\(X_3=0\)</span>, and <span class="math inline">\(X_4=-3\)</span> will have an outcome of <span class="math inline">\(Y=0.118\)</span>.</p>
<html>
<body>
<h2 style="color:#c30a0a">
<strong>Step 10 and beyond…
</h1>
</strong>
</body>
</html>
<p>We could compute the MSE of the ensemble superlearner predictions.</p>
<pre class="r"><code>sl_mse &lt;- mean((obs$y - sl_predictions)^2)
sl_mse</code></pre>
<pre><code>## [1] 0.01927392</code></pre>
<p>We could also add more algorithms to our base learner library (we definitely should, since we only used linear regressions!), and we could write functions to tune these algorithms’ hyperparameters over various grids. For example, if we were to include random forest in our library, we may want to tune over a number of trees and maximum bucket sizes.</p>
<p>We can then cross-validate this entire process to evaluate the predictive performance of our superlearner algorithm. Alternatively, we could leave a hold-out training data set to evaluate the performance.</p>
</div>
</div>
<div id="using-the-superlearner-package" class="section level1">
<h1>Using the <code>SuperLearner</code> package</h1>
<p>Or… we could use a package and avoid all the hand-coding. Here is how you would build an ensemble superlearner for our data with the base learner libraries of <code>ranger</code> (random forests), <code>glmnet</code> (LASSO, by default), and <code>earth</code> (MARS) using the <code>SuperLearner</code> package in <code>R</code>:</p>
<pre class="r"><code>library(SuperLearner)
x_df &lt;- obs %&gt;% select(x1:x4) %&gt;% as.data.frame()
sl_fit &lt;- SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&quot;SL.ranger&quot;, &quot;SL.glmnet&quot;, &quot;SL.earth&quot;))</code></pre>
<p>You can specify the metalearner with the <code>method</code> argument. The default is <a href="##non-negative-least-squares">Non-Negative Least Squares</a> (NNLS).</p>
<div id="cv-risk-and-coefficient-weights" class="section level2">
<h2>CV-Risk and Coefficient Weights</h2>
<p>We can examine the cross-validated <code>Risk</code> (loss function), and the <code>Coef</code>ficient (weight) given to each of the models.</p>
<pre class="r"><code>sl_fit</code></pre>
<pre><code>## 
## Call:  
## SuperLearner(Y = obs$y, X = x_df, family = gaussian(), SL.library = c(&quot;SL.ranger&quot;,  
##     &quot;SL.glmnet&quot;, &quot;SL.earth&quot;)) 
## 
## 
##                      Risk      Coef
## SL.ranger_All 0.013278476 0.1619231
## SL.glmnet_All 0.097149642 0.0000000
## SL.earth_All  0.003168299 0.8380769</code></pre>
<p>From this summary we can see that the CV-risk (the default risk is MSE) in this library of base learners is smallest for <code>SL.Earth</code>. This translates to the largest coefficient, or weight, given to the predictions from <code>earth</code>.</p>
<p>The LASSO model implemented by <code>glmnet</code> has the largest CV-risk, and after the metalearning step, those predictions receive a coefficient, or weight, of 0. This means that the predictions from LASSO will not be incorporated into the final predictions at all.</p>
</div>
<div id="obtaining-the-predictions" class="section level2">
<h2>Obtaining the predictions</h2>
<p>We can extract the predictions easily via the <code>SL.predict</code> element of the <code>SuperLearner</code> fit object.</p>
<pre class="r"><code>kable(head(sl_fit$SL.predict), digits=2, col.names = &quot;sl_predictions&quot;, caption = &quot;Final SL predictions (package)&quot;) </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-17">Table 6: </span>Final SL predictions (package)</caption>
<thead>
<tr class="header">
<th align="right">sl_predictions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.29</td>
</tr>
<tr class="even">
<td align="right">-1.19</td>
</tr>
<tr class="odd">
<td align="right">-0.68</td>
</tr>
<tr class="even">
<td align="right">-0.87</td>
</tr>
<tr class="odd">
<td align="right">-0.97</td>
</tr>
<tr class="even">
<td align="right">-1.08</td>
</tr>
</tbody>
</table>
</div>
<div id="cross-validated-superlearner" class="section level2">
<h2>Cross-validated Superlearner</h2>
<p>Recall that we can cross-validate the entire model fitting process to evaluate the predictive performance of our superlearner algorithm. This is easy with the function <code>CV.SuperLearner()</code>. Beware, this gets computationally burdensome very quickly!</p>
<pre class="r"><code>cv_sl_fit &lt;- CV.SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&quot;SL.ranger&quot;, &quot;SL.glmnet&quot;, &quot;SL.earth&quot;))</code></pre>
<p>For more information on the <code>SuperLearner</code> package, take a look at this <a href="https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html">vignette</a>.</p>
</div>
<div id="alternative-packages-to-superlearn" class="section level2">
<h2>Alternative packages to superlearn</h2>
<p>Other packages freely available in <code>R</code> that can be used to implement the superlearner algorithm include <a href="https://tlverse.org/tlverse-handbook/sl3.html"><code>sl3</code></a> (an update to the original <code>Superlearner</code> package), <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html"><code>h2o</code></a>, and <a href="https://rdrr.io/cran/caretEnsemble/f/vignettes/caretEnsemble-intro.Rmd"><code>caretEnsemble</code></a>. I previously wrote a <a href="https://www.khstats.com/blog/sl3_demo/sl/">brief demo</a> on using <code>sl3</code> for an NYC R-Ladies demo.</p>
<p>The authors of <code>tidymodels</code> – a suite of packages for machine learning including <code>recipes</code>, <code>parsnip</code>, and <code>rsample</code> – recently came out with a new package to perform superlearning/stacking called <a href="%5Bhttps://stacks.tidymodels.org/articles/basics.html"><code>stacks</code></a>. Prior to this, Alex Hayes wrote a <a href="https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/">blog post</a> on using <code>tidymodels</code> infrastructure to implement superlearning.</p>
</div>
</div>
<div id="coming-soon-when-prediction-is-not-the-end-goal" class="section level1">
<h1>Coming soon… when prediction is not the end goal</h1>
<p>When prediction is not the end goal, superlearning combines well with semi-parametric estimation methods for statistical inference. This is the reason I was reading <em>Targeted Learning</em> in the first place; I am a statistician with collaborators who typically want estimates of treatment effects with confidence intervals, not predictions!</p>
<p>I’m working on a similar visual guide for one such semiparametric estimation method: <a href="https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf">Targeted Maximum Likelihood Estimation</a> (TMLE)). TMLE allows the use of flexible statistical models like the superlearner algorithm when estimating treatment effects. If you found this superlearning tutorial helpful, check back here later for another one on TMLE. If you’re curious about TMLE in the meantime, I really like <a href="https://migariane.github.io/TMLE.nb.html">this tutorial</a> by Miguel Angel Luque Fernandez.</p>
<html>
<head>
<title>
HTML Image as link
</title>
</head>
<body>
<a href="https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf">
<img alt="cheatsheet" src="/img/TMLE.jpg"
         width=100%">
</a>
</body>
</html>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<p>These sections contain a bit of extra information on the superlearning algorithm, such as: intuition on manually computing the loss function of interest, explanation of the discrete superlearner, brief advice on choosing a metalearner, and a different summary visual provided in the <em>Targeted Learning</em> book.</p>
<div id="manually-computing-the-mse" class="section level3">
<h3>Manually computing the MSE</h3>
<p>Let’s say we have chosen our loss function of interest to be the Mean Squared Error (MSE). We could first compute the squared error <span class="math inline">\((y - \hat{y})^2\)</span> for each CV prediction A, B, and C.</p>
<pre class="r"><code>cv_sq_error &lt;-
  obs_preds %&gt;%
  mutate(cv_sqrd_error_a = (y-pred_a)^2,   # compute squared error for each observation
         cv_sqrd_error_b = (y-pred_b)^2,
         cv_sqrd_error_c = (y-pred_c)^2)</code></pre>
<pre class="r"><code>cv_sq_error %&gt;% 
  pivot_longer(c(cv_sqrd_error_a, cv_sqrd_error_b, cv_sqrd_error_c), # make the CV squared errors long form for plotting
               names_to = &quot;base_learner&quot;,
               values_to = &quot;squared_error&quot;) %&gt;%
  mutate(base_learner = toupper(str_remove(base_learner, &quot;cv_sqrd_error_&quot;))) %&gt;%
  ggplot(aes(base_learner, squared_error, col=base_learner)) + # make box plots
  geom_boxplot() +
  theme_bw() +
  guides(col=F) +
  labs(x = &quot;Base Learner&quot;, y=&quot;Squared Error&quot;, title=&quot;Squared Errors of Learner A, B, and C&quot;)</code></pre>
<p><img src="/blog/sl/superlearning_files/figure-html/unnamed-chunk-20-1.png" width="528" /></p>
<p>And then take the mean of those three cross-validated squared error columns, grouped by <code>cv_fold</code>, to get the CV-MSE for each fold.</p>
<pre class="r"><code>cv_risks &lt;-
  cv_sq_error %&gt;%
  group_by(cv_fold) %&gt;%
  summarise(cv_mse_a = mean(cv_sqrd_error_a),
            cv_mse_b = mean(cv_sqrd_error_b),
            cv_mse_c = mean(cv_sqrd_error_c)
            )</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>cv_risks %&gt;%
  pivot_longer(cv_mse_a:cv_mse_c,
               names_to = &quot;base_learner&quot;,
               values_to = &quot;mse&quot;) %&gt;%
  mutate(base_learner = toupper(str_remove(base_learner,&quot;cv_mse_&quot;)))  %&gt;%
  ggplot(aes(cv_fold, mse, col=base_learner)) +
  geom_point() +
  theme_bw()  +
    scale_x_continuous(breaks = 1:10) +
  labs(x = &quot;Cross-Validation (CV) Fold&quot;, y=&quot;Mean Squared Error (MSE)&quot;, col = &quot;Base Learner&quot;, title=&quot;CV-MSEs for Base Learners A, B, and C&quot;)</code></pre>
<p><img src="/blog/sl/superlearning_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>We see that across each fold, Learner B consistently has an MSE around 0.02, while Learner C hovers around 0.1, and Learner A varies between 0.35 and 0.45. We can take another mean to get the overall CV-MSE for each learner.</p>
<pre class="r"><code>cv_risks %&gt;%
  select(-cv_fold) %&gt;%
  summarise_all(mean) %&gt;%
  kable(digits=2, caption = &quot;CV-MSE for each base learner&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-22">Table 7: </span>CV-MSE for each base learner</caption>
<thead>
<tr class="header">
<th align="right">cv_mse_a</th>
<th align="right">cv_mse_b</th>
<th align="right">cv_mse_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.38</td>
<td align="right">0.02</td>
<td align="right">0.11</td>
</tr>
</tbody>
</table>
<p>The base learner that performs the best using our chosen loss function of interest is clearly Learner B. We can see from our data simulation code why this is true – Learner B is almost exactly the mimicking the data generating mechanism of <code>y</code>.</p>
<p>Our results align with the linear regression fit from our metalearning step; Learner B predictions received a much larger coefficient relative to Learners A and C.</p>
</div>
<div id="discrete-superlearner" class="section level3">
<h3>Discrete Superlearner</h3>
<p>We <em>could</em> stop after minimizing our loss function (MSE) and fit Learner B to our full data set, and that would be called using the <strong>discrete superlearner</strong>.</p>
<pre class="r"><code>discrete_sl_predictions &lt;- predict(glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs))</code></pre>
<p>However, we can almost always create an even better prediction algorithm if we use information from <em>all</em> of the algorithms’ CV predictions.</p>
</div>
<div id="choosing-a-metalearner" class="section level3">
<h3>Choosing a metalearner</h3>
<p>In the <a href="#references">Reference</a> papers on superlearning, the metalearner which yields the best results theoretically and in practice is a <strong>convex combination optimization</strong> of learners. This means fitting the following regression, where <span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, and <span class="math inline">\(\alpha_3\)</span> are all non-negative and sum to 1.</p>
<p><span class="math inline">\(\mathrm{E}[Y|\hat{Y}_{LrnrA},\hat{Y}_{LrnrB},\hat{Y}_{LrnrC}] = \alpha_1\hat{Y}_{LrnrA} + \alpha_2\hat{Y}_{LrnrB} + \alpha_3\hat{Y}_{LrnrC}\)</span></p>
<p>The default in the <code>Superlearner</code> package is to fit a non-negative least squares (NNLS) regression. NNLS fits the above equation where the <span class="math inline">\(\alpha\)</span>’s must be greater than or equal to 0 but do not necessarily sum to 1. The package then reweights the <span class="math inline">\(\alpha\)</span>’s to force them to sum to 1. This makes the weights a convex combination, but may not yield the same optimal results as an initial convex combination optimization.</p>
<p>The metalearner should change with the goals of the prediction algorithm and the loss function of interest. In these examples it is the MSE, but if the goal is to build a prediction algorithm that is best for binary classification, the loss function of interest may be the rank loss, or <span class="math inline">\(1-AUC\)</span>. It is outside the scope of this post, but for more information, I recommend this <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4912128/">paper</a> by Erin Ledell on maximizing the Area Under the Curve (AUC) with superlearner algorithms.</p>
</div>
<div id="another-visual-guide-for-superlearning" class="section level3">
<h3>Another visual guide for superlearning</h3>
<p>The steps of the superlearner algorithm are summarized nicely in this graphic in Chapter 3 of the <em>Targeted Learning</em> book:</p>
<p><img src="/img/sl_diagram.png" /></p>
</div>
</div>
<div id="acknowledgments" class="section level1">
<h1>Acknowledgments</h1>
<p>Thank you to Eric Polley, Iván Díaz, Nick Williams, Anjile An, and Adam Peterson for very helpful content (and design!) suggestions for this post.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>MJ Van der Laan, EC Polley, AE Hubbard, Super Learner, Statistical applications in genetics and molecular, 2007</p>
<p>Polley, Eric. “Chapter 3: Superlearning.” Targeted Learning: Causal Inference for Observational and Experimental Data, by M. J. van der. Laan and Sherri Rose, Springer, 2011.</p>
<p>Polley E, LeDell E, Kennedy C, van der Laan M. Super Learner: Super Learner Prediction. 2016 URL <a href="https://CRAN.R-project.org/package=SuperLearner" class="uri">https://CRAN.R-project.org/package=SuperLearner</a>. R package version 2.0-22.</p>
<p>Naimi AI, Balzer LB. Stacked generalization: an introduction to super learning. <em>Eur J Epidemiol.</em> 2018;33(5):459-464. <a href="doi:10.1007/s10654-018-0390-z" class="uri">doi:10.1007/s10654-018-0390-z</a></p>
<p>LeDell, E. (2015). Scalable Ensemble Learning and Computationally Efficient Variance Estimation. UC Berkeley. ProQuest ID: LeDell_berkeley_0028E_15235. Merritt ID: ark:/13030/m5wt1xp7. Retrieved from <a href="https://escholarship.org/uc/item/3kb142r2" class="uri">https://escholarship.org/uc/item/3kb142r2</a></p>
<p>M. Petersen and L. Balzer. Introduction to Causal Inference. UC Berkeley, August 2014. <a href="www.ucbbiostat.com">www.ucbbiostat.com</a></p>
</div>
<div id="session-info" class="section level1">
<h1>Session Info</h1>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            knitr_1.28         
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.3.1         tidyr_1.1.2        
## [10] tibble_3.0.3        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5         lubridate_1.7.9    lattice_0.20-38    plotmo_3.5.7      
##  [5] earth_5.1.2        assertthat_0.2.1   glmnet_3.0-2       digest_0.6.25     
##  [9] foreach_1.5.0      ranger_0.12.1      R6_2.4.1           cellranger_1.1.0  
## [13] backports_1.1.8    reprex_0.3.0       evaluate_0.14      httr_1.4.1        
## [17] highr_0.8          blogdown_0.19      pillar_1.4.6       TeachingDemos_2.12
## [21] rlang_0.4.7        readxl_1.3.1       rstudioapi_0.11    Matrix_1.2-18     
## [25] rmarkdown_2.1      labeling_0.3       munsell_0.5.0      broom_0.7.0       
## [29] compiler_3.6.3     modelr_0.1.6       xfun_0.14          pkgconfig_2.0.3   
## [33] shape_1.4.4        htmltools_0.4.0    tidyselect_1.1.0   bookdown_0.19     
## [37] codetools_0.2-16   fansi_0.4.1        crayon_1.3.4       dbplyr_1.4.3      
## [41] withr_2.2.0        cabinets_0.6.0     grid_3.6.3         jsonlite_1.6.1    
## [45] gtable_0.3.0       lifecycle_0.2.0    DBI_1.1.0          magrittr_1.5      
## [49] scales_1.1.1       cli_2.0.2          stringi_1.4.6      farver_2.0.3      
## [53] fs_1.4.1           xml2_1.3.0         ellipsis_0.3.1     generics_0.0.2    
## [57] vctrs_0.3.4        Formula_1.2-3      iterators_1.0.12   tools_3.6.3       
## [61] glue_1.4.2         hms_0.5.3          plotrix_3.7-7      yaml_2.2.1        
## [65] colorspace_1.4-1   rvest_0.3.5        haven_2.2.0</code></pre>
</div>
