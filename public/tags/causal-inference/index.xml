<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>causal inference | KHstats</title>
    <link>https://khstats.com/tags/causal-inference/</link>
      <atom:link href="https://khstats.com/tags/causal-inference/index.xml" rel="self" type="application/rss+xml" />
    <description>causal inference</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 11 Dec 2020 02:13:14 -0500</lastBuildDate>
    <image>
      <url>https://khstats.com/img/icon-192.png</url>
      <title>causal inference</title>
      <link>https://khstats.com/tags/causal-inference/</link>
    </image>
    
    <item>
      <title>An Illustrated Guide to TMLE, Part III: Properties, Theory, and Learning More</title>
      <link>https://khstats.com/blog/tmle/tutorial-pt3/</link>
      <pubDate>Fri, 11 Dec 2020 02:13:14 -0500</pubDate>
      <guid>https://khstats.com/blog/tmle/tutorial-pt3/</guid>
      <description>
&lt;link href=&#34;https://khstats.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://khstats.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;blockquote&gt;
&lt;p&gt;The is the third and final post in a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). In this section, I discuss more &lt;a href=&#34;#properties-of-tmle&#34;&gt;&lt;strong&gt;statistical properties of TMLE&lt;/strong&gt;&lt;/a&gt;, offer a brief &lt;a href=&#34;#why-does-tmle-work&#34;&gt;&lt;strong&gt;explanation for the theory behind TMLE&lt;/strong&gt;&lt;/a&gt;, and provide &lt;a href=&#34;#resources-to-learn-more&#34;&gt;&lt;strong&gt;resources for learning more&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;div id=&#34;properties-of-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Properties of TMLE üìà&lt;/h1&gt;
&lt;p&gt;To reiterate a point from &lt;em&gt;Parts I&lt;/em&gt; and &lt;em&gt;II&lt;/em&gt;, a main motivation for TMLE is that it &lt;strong&gt;allows the use of machine learning algorithms while still yielding asymptotic properties for inference&lt;/strong&gt;. This is notably &lt;em&gt;not&lt;/em&gt; true for many estimators.&lt;/p&gt;
&lt;p&gt;For example, in &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2&#34;&gt;&lt;em&gt;Part II&lt;/em&gt;&lt;/a&gt; we walked through TMLE for the Average Treatment Effect (ATE). Two frequently used alternatives to estimating the ATE are G-computation and Inverse Probability of Treatment Weighting (see &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/#step-1-estimate-the-outcome&#34;&gt;&lt;em&gt;Part II, Step 1&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;#resources-to-learn-more&#34;&gt;&lt;em&gt;references&lt;/em&gt;&lt;/a&gt;). In general, neither yield valid standard errors unless &lt;em&gt;a-priori&lt;/em&gt; specified parametric models are used, and this reliance on parametric assumptions can bias results. There are many simulation studies that show this.&lt;/p&gt;
&lt;p&gt;Another beneficial property of TMLE is that it is a &lt;strong&gt;doubly robust&lt;/strong&gt; estimator. This means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are &lt;strong&gt;consistent&lt;/strong&gt;), the final TMLE estimate will be consistent.&lt;/p&gt;
&lt;p&gt;If both regressions are consistent, the &lt;strong&gt;final estimate will reach the smallest possible variance at a rate of &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}\)&lt;/span&gt;&lt;/strong&gt;, which is the fastest possible rate of convergence and equivalent to parametric maximum likelihood estimation. The reason we use superlearning for estimating the outcome and treatment regressions is to give us the best possible chance of having two correctly specified models and obtaining an &lt;strong&gt;efficient estimate&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle_props.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even among other doubly robust estimators, TMLE is appealing because its estimates will always stay within the bounds of the original outcome. This is because it is part of a class of &lt;strong&gt;substitution estimators&lt;/strong&gt;. There is another class of doubly robust, semiparametric estimation methods frequently used in causal inference that are referred to as &lt;strong&gt;one-step estimators&lt;/strong&gt;, but they can sometimes yield final estimates that are outside the original outcome scale. The one-step estimator for the ATE is called &lt;strong&gt;Augmented Inverse Probability Weighting (AIPW)&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-does-tmle-work&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why does TMLE work? ‚ú®&lt;/h1&gt;
&lt;p&gt;Truly understanding why TMLE works requires semiparametric theory that falls far outside the scope of this tutorial. However, the theory is interesting, so I‚Äôll give a brief, high-level explanation, and then you can look at the &lt;a href=&#34;#resources-to-learn-more&#34;&gt;references&lt;/a&gt; if you‚Äôre curious to learn more. Importantly, the explanation I outline here is more than sufficient and certainly not necessary to appropriately implement TMLE as an analyst.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TMLE relies on the following ideas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Some estimands allow for &lt;strong&gt;asymptotically linear estimation&lt;/strong&gt;. This means that estimators can be represented as sample averages (plus a term that converges to zero).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The quantities being averaged for asymptotically linear estimators are called &lt;strong&gt;influence functions&lt;/strong&gt;. An influence function is a function that quantifies how much influence each observation has on the estimator. For this reason, it is very &lt;strong&gt;useful to characterize the variance of the estimator&lt;/strong&gt;. In parametric maximum likelihood estimation, the influence function is related the score function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;efficient influence function&lt;/strong&gt; (EIF) is the influence function that achieves the efficiency bound (think Cramer Rao Lower Bound from parametric maximum likelihood estimation) and &lt;strong&gt;can be used to create efficient estimators.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we want to &lt;strong&gt;construct an estimator that is efficient&lt;/strong&gt;, we can take advantage of the EIF to endow the estimator with useful asymptotic properties.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is the reason TMLE allows us to use machine learning models ‚Äúunder the hood‚Äù while still obtaining asymptotic properties for inference: our &lt;strong&gt;estimand&lt;/strong&gt; of interest admits &lt;strong&gt;asymptotically linear estimation&lt;/strong&gt;, and we are &lt;strong&gt;using properties of the EIF&lt;/strong&gt; to &lt;strong&gt;construct an estimator&lt;/strong&gt; with &lt;strong&gt;optimal statistical properties&lt;/strong&gt; (e.g.¬†double robustness).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resources-to-learn-more&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Resources to learn more&lt;/h1&gt;
&lt;!-- ![](/img/tmle/self_portrait.jpg){width=40%} --&gt;
&lt;html&gt;
&lt;head&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;&#34;&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/self_portrait.jpg&#34;
               style=&#34;float:right; padding-right:20px; padding-top:-20px; padding-left:20px; padding-bottom:-50px;&#34; width=&#34;34%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;I could only cover so much in this post, but here are the resources I‚Äôve used the most to learn about TMLE, semiparametric estimation, and causal inference. If you are new to any or all of it, there is a good chance it will take &lt;em&gt;several&lt;/em&gt; reads of these materials before the concepts begin to make any sense. Don‚Äôt get discouraged!&lt;/p&gt;
&lt;div id=&#34;tmle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TMLE&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The paper I referred to most often while learning TMLE was &lt;a href=&#34;https://academic.oup.com/aje/article/185/1/65/2662306&#34;&gt;&lt;em&gt;Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies&lt;/em&gt;&lt;/a&gt; by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and details the statistical advantages of TMLE for an applied thinker.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also really like the written explanations in the &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; book (Chapters 4 and 5) by Mark van der Laan and Sherri Rose. The notation was often too difficult for me to follow, but the words themselves make a lot of sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Miguel Angel Luque-Fernandez wrote an &lt;a href=&#34;https://migariane.github.io/TMLE.nb.html&#34;&gt;excellent bookdown tutorial on TMLE&lt;/a&gt;, also with step-by-step &lt;code&gt;R&lt;/code&gt; code. It is more technical and thorough than my post, but still aimed at an applied audience. He also has a tutorial on the &lt;a href=&#34;https://migariane.github.io/DeltaMethodEpiTutorial.nb.html&#34;&gt;functional delta method&lt;/a&gt; which is part of the theory behind the way we compute the standard errors (see &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/#step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values&#34;&gt;&lt;em&gt;Part II, Step 6&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other code-based web-based tutorials on TMLE include David Benkeser and Antoine Chambaz‚Äô &lt;a href=&#34;https://achambaz.github.io/tlride/&#34;&gt;A Ride in Targeted Learning Territory&lt;/a&gt; and the authors of the &lt;code&gt;R&lt;/code&gt; package suite &lt;a href=&#34;https://tlverse.org/&#34;&gt;&lt;code&gt;tlverse&lt;/code&gt;&lt;/a&gt;‚Äôs &lt;a href=&#34;https://tlverse.org/tlverse-handbook/tmle3.html&#34;&gt;Hitchhiker‚Äôs Guide to Targeted Learning: The TMLE Framework&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;semiparametric-theory-and-influence-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Semiparametric Theory and Influence Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Edward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;His introductory paper on &lt;a href=&#34;https://arxiv.org/pdf/1709.06418.pdf&#34;&gt;Semiparametric Theory&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;His &lt;a href=&#34;http://www.ehkennedy.com/uploads/5/8/4/5/58450265/unc_2019_cirg.pdf&#34;&gt;slideshow tutorial&lt;/a&gt; &lt;em&gt;Nonparametric efficiency theory and machine learning in causal inference&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My favorite resource so far for learning specifically about influence functions has been &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1717620&#34;&gt;Visually Communicating Influence Functions&lt;/a&gt; by Aaron Fisher and Edward Kennedy. However, this paper didn‚Äôt make sense to me until I worked through this &lt;a href=&#34;https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives&#34;&gt;interactive tutorial&lt;/a&gt; by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The derivation of the Efficient Influence Function (EIF) in TMLE is in the Appendix of &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causal Inference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;As emphasized in &lt;em&gt;Part I&lt;/em&gt;, TMLE is an estimation technique which &lt;em&gt;can&lt;/em&gt; be used for causal inference. If you want to learn about the foundations of causal inference, I suggest two different introductory texts below. Note that these provide fairly different frameworks (notation, descriptions of assumptions) to reach the same conclusions, but both provide useful perspectives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://bayes.cs.ucla.edu/PRIMER/&#34;&gt;&lt;em&gt;Causal Inference in Statistics: A Primer&lt;/em&gt;&lt;/a&gt; by Judea Pearl. Pearl does not discuss estimation methods, but rather focuses on the assumptions, or identification, side of causal inference. Thus, you will not find TMLE mentioned in this text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;&lt;em&gt;What If&lt;/em&gt;&lt;/a&gt; by Miguel Hernan and James Robins. Notably, Hernan and Robins only discuss parametric estimation methods, so you will also not find TMLE or AIPW in this text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also think the introductory chapters of the previously mentioned &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; book (Chapters 1 and 2) do an excellent job of setting up the ‚Äúroadmap‚Äù of causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I‚Äôll continue to update this page with beginner‚Äôs resources as I discover them.&lt;/p&gt;
&lt;p&gt;Feedback or clarifications on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through &lt;a href=&#34;mailto:kathoffman.stats@gmail.com&#34;&gt;email&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;This tutorial would not have been possible without my colleague Iv√°n D√≠az patiently answering many, many questions on TMLE. I am also very appreciative of Miguel Angel Luque-Fernandez‚Äôs helpful feedback on the visual guide.&lt;/p&gt;
&lt;p&gt;Lastly, many thanks to Axel Martin, Nick Williams, Anjile An, Adam Peterson, Alan Wu, and Will Simmons for providing suggestions on various drafts of this art project!&lt;/p&gt;
&lt;!-- ![](/img/tmle/self_portrait.jpg){width=45%} --&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Illustrated Guide to TMLE, Part II: The Algorithm</title>
      <link>https://khstats.com/blog/tmle/tutorial-pt2/</link>
      <pubDate>Fri, 11 Dec 2020 01:13:14 -0500</pubDate>
      <guid>https://khstats.com/blog/tmle/tutorial-pt2/</guid>
      <description>
&lt;link href=&#34;https://khstats.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://khstats.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;html&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://khstats.com/style.css&#34; /&gt;
&lt;/html&gt;
&lt;blockquote&gt;
&lt;p&gt;The second post of a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). This section walks through the &lt;strong&gt;TMLE algorithm for the mean difference in outcomes for a binary treatment and binary outcome&lt;/strong&gt;. &lt;br&gt; &lt;br&gt; This post is an expansion of a printable ‚Äúvisual guide‚Äù available on my &lt;a href=&#34;https://github.com/hoffmakl/causal-inference-visual-guides&#34;&gt;Github&lt;/a&gt;. I hope it helps analysts who feel out-of-practice reading mathematical notation follow along with the TMLE algorithm. A web-based key without explanations is also available &lt;a href=&#34;https://khstats.com/blog/tmle/visual-key&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;https://khstats.com/img/tmle.jpg&#34;
         width=100%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;hr /&gt;
&lt;p&gt;‚¨ÖÔ∏è&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial/&#34;&gt;&lt;em&gt;Return to Part I: Motivation&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;part-ii-the-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part II: The Algorithm:&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;#why-the-visual-guide&#34;&gt;Why the Visual Guide? üé®&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#tmle-step-by-step&#34;&gt;TMLE, Step-by-Step üö∂üèΩÔ∏è&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-1-estimate-the-outcome&#34;&gt;Estimate the Outcome&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-2-estimate-the-probability-of-treatment&#34;&gt;Estimate the Probability of Treatment&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-3-estimate-the-fluctuation-parameter&#34;&gt;Estimate the Fluctuation Parameter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-4-update-the-initial-estimates-of-the-expected-outcome&#34;&gt;Update the Initial Estimates of the Expected Outcome&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-5-compute-the-statistical-estimand-of-interest&#34;&gt;Compute the Statistical Estimand of Interest&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values&#34;&gt;Calculate the Standard Errors for Confidence Intervals and P-values&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;#using-the-tmle-package&#34;&gt;Using the &lt;code&gt;tmle&lt;/code&gt; package üì¶&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#full-tutorial-code&#34;&gt;Full tutorial code üíª&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‚û°Ô∏è&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/&#34;&gt;&lt;em&gt;Skip to Part III: Evaluation&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;why-the-visual-guide&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why the Visual Guide? üé®&lt;/h1&gt;
&lt;p&gt;TMLE was developed in 2007 by Mark van der Laan and colleagues at UC Berkeley, and it is slowly but surely starting to see more widespread use. Since learning about TMLE, I‚Äôve believed many more analysts with a skill set similar to mine &lt;em&gt;could&lt;/em&gt; be using TMLE, but perhaps find even the most introductory resources to be a bit daunting.&lt;/p&gt;
&lt;p&gt;I am a very applied thinker, and I‚Äôve tried to write this tutorial in the way I would have found most useful and accessible when I began learning about TMLE. Each step is accompanied by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a non-rigorous explanation&lt;/li&gt;
&lt;li&gt;an equation using the simplest notation possible&lt;/li&gt;
&lt;li&gt;in-line &lt;code&gt;R&lt;/code&gt; code&lt;/li&gt;
&lt;li&gt;a small graphic representing the computation on a data frame or vector&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This last piece is super important for me, because I remember best when I associate an image with what I‚Äôve learned. If the graphics are also useful to you, it may be helpful to open this &lt;a href=&#34;https://khstats.com/blog/tmle/visual-key&#34;&gt;corresponding key&lt;/a&gt; in another browser or print the &lt;a href=&#34;https://github.com/hoffmakl/causal-inference-visual-guides&#34;&gt;visual guide&lt;/a&gt; for reference.&lt;/p&gt;
&lt;p&gt;This tutorial is not meant to replace the resources I used to learn TMLE, but rather to supplement them. I use the same mathematical notation as the TMLE literature to make it easier to move back and forth. I hope you find the way I think about the algorithm useful, but if not, consider checking out the references I‚Äôve listed in &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/#resources-to-learn-more&#34;&gt;&lt;em&gt;Part III&lt;/em&gt;&lt;/a&gt;. TMLE is a very powerful method and will undoubtedly only grow in popularity in statistics and data science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tmle-step-by-step&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TMLE, Step-by-Step üö∂üèΩ&lt;/h1&gt;
&lt;p&gt;Let‚Äôs look at the algorithm step-by-step. As you‚Äôre reading, keep in mind that there are &lt;code&gt;R&lt;/code&gt; packages that will do this for you as easily as running a &lt;code&gt;glm()&lt;/code&gt; or &lt;code&gt;coxph()&lt;/code&gt; function. This tutorial is to help understand what‚Äôs going on behind-the-scenes.&lt;/p&gt;
&lt;p&gt;One final note before we dive into the algorithm:&lt;/p&gt;
&lt;div id=&#34;superlearning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Superlearning&lt;/h3&gt;
&lt;p&gt;I use the ensemble learning method superlearning (also known as ‚Äústacking‚Äù) to demonstrate TMLE. This is because superlearning is theoretically and empirically proven to yield the best results in TMLE.&lt;/p&gt;
&lt;p&gt;If you‚Äôre new to superlearning/stacking, the necessary knowledge for this post is that it allows you to combine many machine learning algorithms for prediction. When I use &lt;code&gt;SuperLearner()&lt;/code&gt; in the following example code, I could have used &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;randomForest()&lt;/code&gt;, or any other parametric or non-parametric supervised learning algorithm.&lt;/p&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;https://khstats.com/blog/sl/superlearning&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;https://khstats.com/img/Superlearning.jpg&#34;  
         width=&#34;50%&#34; style=&#34;float:right; padding-left:60px;&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;Note that the choice of base learners for superlearning is not the point of this post. In practice we simply want to add flexible machine learning models which we think &lt;strong&gt;may&lt;/strong&gt; reflect the underlying data distribution. For a tutorial on superlearning, you can check out one of my &lt;a href=&#34;https://khstats.com/blog/sl/superlearning&#34;&gt;previous blog posts&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-set-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Initial set up&lt;/h3&gt;
&lt;p&gt;Let‚Äôs first load the necessary libraries and set a seed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for data manipulation
library(kableExtra) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let‚Äôs simulate a data set for demonstration of the algorithm. This data will have a very simple structure: a binary treatment, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;; binary outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;; and four confounders: &lt;span class=&#34;math inline&#34;&gt;\(W_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_3\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(W_4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/1_data_structure.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_data &amp;lt;- function(n){ 
    W1 &amp;lt;- rbinom(n, size=1, prob=0.2) # binary confounder
    W2 &amp;lt;- rbinom(n, size=1, prob=0.5) # binary confounder
    W3 &amp;lt;- round(runif(n, min=2, max=7)) # continuous confounder
    W4 &amp;lt;- round(runif(n, min=0, max=4)) # continuous confounder
    A  &amp;lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders
    Y &amp;lt;- rbinom(n, size=1, prob= plogis(-1 + A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4))) # binary outcome depends on confounders
    return(tibble(Y, W1, W2, W3, W4, A))
}

n &amp;lt;- 1000
dat_obs &amp;lt;- generate_data(n) # generate a data set with n observations

kable(head(dat_obs), digits=2, caption = &amp;quot;Simulated data set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Simulated data set.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W4
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As mentioned earlier, TMLE can estimate many different statistical estimands of interest. In this example, our statistical estimand is the mean difference in outcomes between those who received the treatment and those who did not, adjusting for confounders.&lt;/p&gt;
&lt;p&gt;Under causal assumptions, this could be &lt;em&gt;identifiable&lt;/em&gt; as the Average Treatment Effect (ATE). I won‚Äôt go through the identification process here (see &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/#resources-to-learn-more&#34;&gt;&lt;em&gt;Part III, References&lt;/em&gt;&lt;/a&gt;), but since TMLE is often used for causal inference, let‚Äôs pretend for this example that we previously met causal assumptions and call our statistical estimand, &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, the ATE.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ATE = \Psi = E_W[\mathrm{E}[Y|A=1,\mathbf{W}] - \mathrm{E}[Y|A=0,\mathbf{W}]]\]&lt;/span&gt;
The ATE, once we estimate it, will be interpretable as the mean difference in outcomes in a hypothetical world where everyone received the treatment compared to a hypothetical world where no one received the treatment.&lt;/p&gt;
&lt;p&gt;At this point in set-up, we should also pick our base machine learning algorithms to combine via superlearning to estimate the expected outcome and probability of treatment. Let‚Äôs use LASSO (&lt;code&gt;glmnet&lt;/code&gt;), random forests (&lt;code&gt;ranger&lt;/code&gt;), Multivariate Adaptive Regression Splines (MARS) (&lt;code&gt;earth&lt;/code&gt;), and a generalized linear model (&lt;code&gt;glm&lt;/code&gt;) for demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_libs &amp;lt;- c(&amp;#39;SL.glmnet&amp;#39;, &amp;#39;SL.ranger&amp;#39;, &amp;#39;SL.earth&amp;#39;, &amp;#39;SL.glm&amp;#39;) # a library of machine learning algorithms (penalized regression, random forests, and multivariate adaptive regression splines)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-estimate-the-outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Estimate the Outcome&lt;/h2&gt;
&lt;p&gt;The very first step of TMLE is to &lt;strong&gt;estimate the expected value of the outcome&lt;/strong&gt; using treatment and confounders as predictors.&lt;/p&gt;
&lt;p&gt;This is what that looks like in mathematical notation. There is some function &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; which takes &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; as inputs and yields the conditional expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(A,\mathbf{W}) = \mathrm{E}[Y|A,\mathbf{W}]\]&lt;/span&gt;
We can use any regression to estimate this conditional expectation, but it is best to use flexible machine learning models so that we don‚Äôt have unnecessary assumptions on the underlying distribution of the data.&lt;/p&gt;
&lt;p&gt;We can think about the above equation as some generic regression function in &lt;code&gt;R&lt;/code&gt; called &lt;code&gt;fit()&lt;/code&gt; with inputs in formula form: &lt;code&gt;outcome ~ predictors&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/2_outcome_fit.png&#34; style=&#34;width:70.0%&#34; /&gt;
In real &lt;code&gt;R&lt;/code&gt; code, we‚Äôll use the &lt;code&gt;SuperLearner()&lt;/code&gt; function to fit a weighted combination of multiple machine learning models (defined earlier in &lt;code&gt;sl_libs&lt;/code&gt;). This function takes the outcome &lt;code&gt;Y&lt;/code&gt; as a vector and a data frame &lt;code&gt;X&lt;/code&gt; as predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Y &amp;lt;- dat_obs$Y
W_A &amp;lt;- dat_obs %&amp;gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner
Q &amp;lt;- SuperLearner(Y = Y, # Y is the outcome vector
                  X = W_A, # W_A is the matrix of W1, W2, W3, W4, and A
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: earth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: ranger&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we should estimate the outcome for every observation under three different scenarios:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. If every observation received the treatment they &lt;em&gt;actually&lt;/em&gt; received.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can get this expected outcome estimate by simply calling &lt;code&gt;predict()&lt;/code&gt; on the model fit without specifying any new data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(A,\mathbf{W}) = \mathrm{\hat{E}}[Y|A,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will save that vector of estimates as a new object in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/3_QA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_A &amp;lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. If every observation received the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To do this, we‚Äôll first need to create a data set where every observation received the treatment of interest, whether they actually did or not. Then we can call the &lt;code&gt;predict()&lt;/code&gt; function on that data set.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(1,\mathbf{W}) = \mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/4_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_A1 &amp;lt;- W_A %&amp;gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &amp;lt;- as.vector(predict(Q, newdata = W_A1)$pred) # predict on that everyone-exposed data set&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. If every observation received the control.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Similarly, we create a data set where every observation did &lt;em&gt;not&lt;/em&gt; receive the treatment of interest, whether they actually did or not, and call the &lt;code&gt;predict()&lt;/code&gt; function again.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(0,\mathbf{W}) = \mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/5_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_A0 &amp;lt;- W_A %&amp;gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &amp;lt;- as.vector(predict(Q, newdata = W_A0)$pred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Next&lt;/em&gt;&lt;/strong&gt;, let‚Äôs create a new data frame, &lt;code&gt;dat_tmle&lt;/code&gt;, to hold the three vectors we‚Äôve created so far, along with the treatment status &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and observed outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Notice that when &lt;span class=&#34;math inline&#34;&gt;\(A=1\)&lt;/span&gt;, the expected outcome &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; equals the expected outcome under treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(A=0\)&lt;/span&gt;, the expected outcome &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; equals the expected outcome under no treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_tmle &amp;lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)
kable(head(dat_tmle), digits=2, caption = &amp;quot;TMLE Algorithm after Step 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 2: &lt;/span&gt;TMLE Algorithm after Step 1
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.70
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.81
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- *Also note that our expected outcomes are on the original outcome scale (i.e. probability, rather than the $logit$ probability).* --&gt;
&lt;p&gt;We could stop here and get our estimate of the ATE by computing the average difference between &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\)&lt;/span&gt;, which &lt;em&gt;would&lt;/em&gt; be the mean difference in the expected outcomes, conditional on confounders. This estimation method is often called &lt;strong&gt;standardization&lt;/strong&gt;, &lt;strong&gt;simple substitution estimation&lt;/strong&gt;, &lt;strong&gt;g-formula estimation&lt;/strong&gt;, or &lt;strong&gt;G-computation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{ATE}_{G-comp}= \hat{\Psi}_{G-comp} = \frac{1}{N}\sum_{i=1}^{N}(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]-\mathrm{\hat{E}}[Y|A=0,\mathbf{W}])\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/ate_gcomp.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ate_gcomp &amp;lt;- mean(dat_tmle$Q_1 - dat_tmle$Q_0)
ate_gcomp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1938238&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this G-computation ATE estimate does not have the appropriate bias-variance tradeoff for the ATE because it was built to have the best bias-variance tradeoff for estimating the &lt;em&gt;outcome&lt;/em&gt;, conditional on confounders, rather than the ATE. We also cannot compute the standard error of the estimator because we don‚Äôt know the sampling distribution of the machine learning estimates.&lt;/p&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-estimate-the-probability-of-treatment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Estimate the Probability of Treatment&lt;/h2&gt;
&lt;p&gt;The next step is to estimate the probability of treatment, given confounders. This quantity is often called the &lt;strong&gt;propensity score&lt;/strong&gt;, as in it gives the probability or &lt;em&gt;propensity&lt;/em&gt; that an observation will receive a treatment of interest.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[g(\mathbf{W}) = \mathrm{Pr}(A=1|\mathbf{W})\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/6_treatment_fit.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Pr}(A=1|\mathbf{W})\)&lt;/span&gt; in the same way as we estimated &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{E}[Y|A,\mathbf{W}]\)&lt;/span&gt;: using the superlearner algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- dat_obs$A
W &amp;lt;- dat_obs %&amp;gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4
g &amp;lt;- SuperLearner(Y = A, # outcome is the A (treatment) vector
                  X = W, # W is a matrix of predictors
                  family=binomial(), # treatment is a binomial outcome
                  SL.library=sl_libs) # using same candidate learners; could use different learners&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we need to compute three different quantities from this model fit:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. The inverse probability of receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(1,\mathbf{W}) = \frac{1}{g(\mathbf{W})} = \frac{1}{\mathrm{Pr}(A=1|\mathbf{W})}\]&lt;/span&gt;
We can estimate the probability of receiving treatment for every observation by using the &lt;code&gt;predict()&lt;/code&gt; funcion without specifying any new data, and then take the inverse of that.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/7_H1.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_w &amp;lt;- as.vector(predict(g)$pred) # Pr(A=1|W)
H_1 &amp;lt;- 1/g_w&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. The negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(0,\mathbf{W}) = -\frac{1}{1-g(\mathbf{W})}= -\frac{1}{\mathrm{Pr}(A=0|\mathbf{W})}\]&lt;/span&gt;
The probability of not receiving treatment for a binary treatment is simply 1 minus the probability of treatment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/8_H0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H_0 &amp;lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment.&lt;/strong&gt; I‚Äôll discuss why later, but in the TMLE literature this is called the &lt;strong&gt;clever covariate&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(A,\mathbf{W}) = \frac{\mathrm{I}(A=1)}{\mathrm{Pr}(A=1|\mathbf{W})}-\frac{\mathrm{I}(A=0)}{\mathrm{Pr}(A=0|\mathbf{W})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/9_HA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To calculate the clever covariate, we‚Äôll first add the &lt;span class=&#34;math inline&#34;&gt;\(H(1,\mathbf{W})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H(0,\mathbf{W})\)&lt;/span&gt; vectors to our &lt;code&gt;dat_tmle&lt;/code&gt; data frame, and then we can use &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to assign &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_tmle &amp;lt;- # add clever covariate data to dat_tmle
  dat_tmle %&amp;gt;%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %&amp;gt;%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our initial estimates of the outcome, and the estimates of the probability of treatment:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(dat_tmle), digits=2, caption=&amp;quot;TMLE Algorithm after Step 2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-14&#34;&gt;Table 3: &lt;/span&gt;TMLE Algorithm after Step 2
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_A
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.42
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.39
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.77
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.13
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-estimate-the-fluctuation-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Estimate the Fluctuation Parameter&lt;/h2&gt;
&lt;p&gt;To reiterate, in &lt;a href=&#34;#step-1-estimate-the-outcome&#34;&gt;Step 1&lt;/a&gt; we estimated the expected outcome, conditional on treatment and confounders. We know those machine learning fits have an optimal bias-variance trade-off for estimating the outcome (conditional on treatment and confounders), rather than the ATE. &lt;strong&gt;We will now use information about the treatment mechanism (from Step 2) to optimize the bias-variance trade-off for the ATE so we can obtain valid inference.&lt;/strong&gt;&lt;/p&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;https://khstats.com/img/bear_with_me.jpg&#34;
               style=&#34;float:right; padding-left:65px;&#34; width=50%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;üö® &lt;strong&gt;&lt;em&gt;Warning:&lt;/em&gt;&lt;/strong&gt; this step is easy to code, but difficult to understand unless you have a background in semiparametric theory. Let‚Äôs first look at &lt;em&gt;what&lt;/em&gt; we‚Äôre doing, and then I‚Äôll discuss at a very high-level &lt;em&gt;why&lt;/em&gt; we‚Äôre doing that in a &lt;a href=&#34;#why-does-tmle-work&#34;&gt;later section&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The point of this step is to solve an &lt;em&gt;estimating equation&lt;/em&gt; for the &lt;em&gt;efficient influence function&lt;/em&gt; (EIF) of our estimand of interest. Without diving into what an EIF or an estimating equation is, let‚Äôs accept for a moment that they will help us:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Update our initial outcome estimates&lt;/strong&gt; so that our estimate of the ATE is asymptotically unbiased (under certain conditions, see &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/#properties-of-tmle&#34;&gt;&lt;em&gt;Part III, Statistical Properties&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Calculate the variance&lt;/strong&gt;, and thus the standard error, confidence interval, and p-value for our estimate of the ATE for hypothesis testing.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next, let‚Äôs take a look at a model that will help us solve an EIF estimating equation and then update our estimates:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[logit(\mathrm{E}[Y|A,\mathbf{W}]) = logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}]) + \epsilon H(A,\mathbf{W})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To reiterate, I haven‚Äôt explained &lt;em&gt;at all&lt;/em&gt; why this step works; we‚Äôre just focusing on implementing it for now.&lt;/p&gt;
&lt;p&gt;If we look at the left side, we can see this equation contains the true outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, just &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; transformed. Luckily for us, there‚Äôs a well-known model that &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; transforms the left side of an equation: logistic regression. Our estimating equation looks &lt;em&gt;a lot&lt;/em&gt; like a simple logistic regression, actually: &lt;span class=&#34;math inline&#34;&gt;\(logit(\mathrm{E}[Y|X]) = \beta_0 + \beta_1 X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Do you see how our equation &lt;em&gt;also&lt;/em&gt; has a vector on the right-hand side, &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt;, with a corresponding coefficient, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;? The only difference is that the ‚Äúintercept‚Äù in our equation, &lt;span class=&#34;math inline&#34;&gt;\(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}])\)&lt;/span&gt; is not a constant value like &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; it is a vector of values. We can see it as an &lt;strong&gt;offset&lt;/strong&gt; or a &lt;strong&gt;fixed intercept&lt;/strong&gt; in a logistic regression, rather than a constant-value intercept.&lt;/p&gt;
&lt;p&gt;Therefore, to accomplish our goal of solving an estimating equation for the EIF we can leverage standard statistical software and fit a logistic regression with one covariate, &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt;, and the initial outcome estimate, &lt;span class=&#34;math inline&#34;&gt;\(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}])\)&lt;/span&gt;, as a fixed intercept. The outcome of the logistic regression is the observed outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Two technical points for application: we use &lt;code&gt;qlogis&lt;/code&gt; to transform the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; to the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale. Also, the &lt;code&gt;R&lt;/code&gt; code for a fixed intercept is &lt;code&gt;-1 + offset(fixed_intercept)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/10_logistic_regression.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note that we are only using a logistic regression because it happens to have the correct form for solving the estimating equation for the EIF for the ATE estimand. It has nothing to do with having a binary outcome, and it isn‚Äôt putting any parametric restraints on our data.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Next we need to save the coefficient from that logistic regression, which we will call &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/11_epsilon.png&#34; style=&#34;width:40.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eps &amp;lt;- coef(glm_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the TMLE literature, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is called the &lt;strong&gt;fluctuation parameter&lt;/strong&gt;, because it provides information about how much to change, or fluctuate, our initial outcome estimates. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt; is called the &lt;strong&gt;clever covariate&lt;/strong&gt; because it ‚Äúcleverly‚Äù helps us solve for the EIF and then update our estimates.&lt;/p&gt;
&lt;p&gt;We will use both the fluctuation parameter and clever covariate in the next step to update our initial estimates of the expected outcome, conditional on confounders and treatment.&lt;/p&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-update-the-initial-estimates-of-the-expected-outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Update the Initial Estimates of the Expected Outcome&lt;/h2&gt;
&lt;p&gt;Almost done! Let‚Äôs recap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In &lt;a href=&#34;#estimate-the-outcome&#34;&gt;Step 1&lt;/a&gt;, we obtained &lt;strong&gt;initial estimates&lt;/strong&gt; of the expected outcome using machine learning (ML). These ML estimates are optimized to estimate &lt;span class=&#34;math inline&#34;&gt;\(E[Y|A,W]\)&lt;/span&gt;, not the ATE.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We need to update those initial expected outcome estimates using &lt;strong&gt;information about the treatment mechanism&lt;/strong&gt;, so we computed the expected probability of treatment, conditional on confounders, in &lt;a href=&#34;#estimate-the-probability-of-treatment&#34;&gt;Step 2&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then, in &lt;a href=&#34;#estimate-the-fluctuation-parameter&#34;&gt;Step 3&lt;/a&gt;, we used quantities from Step 1 and Step 2 to &lt;strong&gt;solve an estimating equation for the EIF&lt;/strong&gt;. We didn‚Äôt talk about &lt;em&gt;why&lt;/em&gt; this works; we simply accepted for now that this is how we can target our estimand of interest (the ATE).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now, we will &lt;strong&gt;update our initial outcome estimates&lt;/strong&gt; from Step 1 using information from Step 2 and 3 to obtain the correct bias-variance tradeoff for the ATE.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To update our expected outcome estimates, we first need to &lt;strong&gt;put the initial expected outcome estimates on the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale&lt;/strong&gt; using &lt;code&gt;qlogis()&lt;/code&gt; because that‚Äôs the scale we used to solve our estimating equation for the EIF in Step 3. Then we can &lt;strong&gt;calculate how much we need to fluctuate our initial estimates&lt;/strong&gt; using the product of the clever covariate and fluctuation parameter: &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W}) \times \hat{\epsilon}\)&lt;/span&gt;. These are our outputs of Step 2 and 3, respectively. We will &lt;strong&gt;add that quantity to the initial outcome estimates to create updated outcome estimates&lt;/strong&gt;. Finally, we can &lt;strong&gt;put the updated estimates back on the true outcome scale&lt;/strong&gt; using &lt;code&gt;plogis()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note we can use &lt;span class=&#34;math inline&#34;&gt;\(expit\)&lt;/span&gt; to show the inverse of the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; function, and we will denote updates to the outcome regressions as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}^*\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}]) + \hat{\epsilon}H(A,\mathbf{W}))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/update_qAW.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H_A &amp;lt;- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H_A)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=1,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]) + \hat{\epsilon}H(A,1))\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/12_update_Q1.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=0,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]) + \hat{\epsilon}H(A,0))\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/13_update_Q0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-compute-the-statistical-estimand-of-interest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5: Compute the Statistical Estimand of Interest&lt;/h2&gt;
&lt;p&gt;We now have updated expected outcomes estimates, so we can &lt;strong&gt;compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{ATE}_{TMLE} = \hat{\Psi}_{TMLE} = \frac{1}{N}\sum_{i=1}^{N}[\hat{E^*}[Y|A=1,\mathbf{W}] - \hat{E^*}[Y|A=0,\mathbf{W}]]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/14_compute_ATE.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_ate &amp;lt;- mean(Q_1_update - Q_0_update)
tmle_ate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1920762&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then say, ‚Äúthe average treatment effect was estimated to be 19.2%.‚Äù&lt;/p&gt;
&lt;p&gt;If causal assumptions were not met, we would say, ‚Äúthe proportion of observations who experienced the outcome, after adjusting for baseline confounders, was estimated to be 19.2% higher for those who received treatment compared to those who did not.‚Äù&lt;/p&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 6: Calculate the Standard Errors for Confidence Intervals and P-values&lt;/h2&gt;
&lt;p&gt;This point estimate is great, but we usually need an &lt;strong&gt;estimate of variance&lt;/strong&gt; so that we can compute confidence intervals, test statistics, p-values, etc. This is another step that contains quite a lot of theory, so I‚Äôll give another birds-eye view for now. If you‚Äôre curious, you can read more about this in &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3&#34;&gt;&lt;em&gt;Part III&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To obtain the standard errors, we first need to compute the &lt;strong&gt;Influence Function&lt;/strong&gt; (IF), which is the empirical version of what we used our estimating equation to figure out in &lt;a href=&#34;#step-3-estimate-the-fluctuation-parameter&#34;&gt;Step 3&lt;/a&gt;. The IF tells us &lt;strong&gt;how much each observation influences the final estimate&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The equation for the IF looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{IF} = (Y-\hat{E^*}[Y|A,\mathbf{W}])H(A,\mathbf{W}) + \hat{E^*}[Y|A=1,\mathbf{W}] - \hat{E^*}[Y|A=0,\mathbf{W}] - \hat{ATE}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;infl_fn &amp;lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the IF, we can &lt;strong&gt;take the square-root of its variance divided by the number of observations&lt;/strong&gt; to get the standard error of our estimate.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{SE} = \sqrt{\frac{var(\hat{IF})}{N}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/15_ses.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_se &amp;lt;- sqrt(var(infl_fn)/nrow(dat_obs))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have that standard error, we can easily get the 95% confidence interval and p-value of our estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_low &amp;lt;- tmle_ate - 1.96*tmle_se
conf_high &amp;lt;- tmle_ate + 1.96*tmle_se
pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))

kable(tibble(tmle_ate, conf_low, conf_high), digits=3, caption = &amp;quot;TMLE Estimate of the ATE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-23&#34;&gt;Table 4: &lt;/span&gt;TMLE Estimate of the ATE
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
tmle_ate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
conf_low
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
conf_high
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.192
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.134
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.25
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Then we can successfully report our ATE as 0.192 (95% CI: 0.134, 0.25).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that a TMLE estimator is asymptotically normally distributed, so we could bootstrap the entire algorithm to get our standard errors instead.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-tmle-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the &lt;code&gt;tmle&lt;/code&gt; package üì¶&lt;/h1&gt;
&lt;p&gt;Luckily there are &lt;code&gt;R&lt;/code&gt; packages so that you don‚Äôt have to hand code TMLE yourself. &lt;code&gt;R&lt;/code&gt; packages to implement the TMLE algorithm include &lt;a href=&#34;https://www.jstatsoft.org/article/view/v051i13&#34;&gt;&lt;code&gt;tmle&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://tlverse.org/tlverse-handbook/tmle3.html&#34;&gt;&lt;code&gt;tmle3&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.jstatsoft.org/article/view/v081i01&#34;&gt;&lt;code&gt;ltmle&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://htmlpreview.github.io/?https://gist.githubusercontent.com/nt-williams/ddd44c48390b8d976fad71750e48d8bf/raw/45db700a02bf92e2a55790e60ed48266a97ca4e7/intro-lmtp.html&#34;&gt;&lt;code&gt;lmtp&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code using the original &lt;code&gt;tmle&lt;/code&gt; package‚Äôs &lt;code&gt;tmle()&lt;/code&gt; function is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_fit &amp;lt;-
  tmle::tmle(Y = Y, # outcome vector
           A = A, # treatment vector
           W = W, # matrix of confounders W1, W2, W3, W4
           Q.SL.library = sl_libs, # superlearning libraries from earlier for outcome regression Q(A,W)
           g.SL.library = sl_libs) # superlearning libraries from earlier for treatment regression g(W)

tmle_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Additive Effect
##    Parameter Estimate:  0.19111
##    Estimated Variance:  0.00088683
##               p-value:  1.3847e-10
##     95% Conf Interval: (0.13275, 0.24948) 
## 
##  Additive Effect among the Treated
##    Parameter Estimate:  0.18898
##    Estimated Variance:  0.00092392
##               p-value:  5.0597e-10
##     95% Conf Interval: (0.1294, 0.24856) 
## 
##  Additive Effect among the Controls
##    Parameter Estimate:  0.19297
##    Estimated Variance:  0.0010143
##               p-value:  1.3687e-09
##     95% Conf Interval: (0.13055, 0.2554)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the same result (variation due to randomness in the machine learning models) in just a few lines of code: the estimate using the original &lt;code&gt;tmle&lt;/code&gt; package is 0.191 (95% CI: 0.133, 0.249).&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;To learn more about the statistical properties of TMLE and the underlying theory, &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3&#34;&gt;&lt;em&gt;continue on to Part III&lt;/em&gt;&lt;/a&gt;. Alternatively, return to the motivation in &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial/&#34;&gt;&lt;em&gt;Part I&lt;/em&gt;&lt;/a&gt;, or see the full tutorial code below.&lt;/p&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#part-ii-the-algorithm&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;full-tutorial-code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Full tutorial code&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for data manipulation
library(kableExtra) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results

sl_libs &amp;lt;- c(&amp;#39;SL.glmnet&amp;#39;, &amp;#39;SL.ranger&amp;#39;, &amp;#39;SL.earth&amp;#39;) # a library of machine learning algorithms (penalized regression, random forests, and multivariate adaptive regression splines)

generate_data &amp;lt;- function(n){ 
    W1 &amp;lt;- rbinom(n, size=1, prob=0.2) # binary confounder
    W2 &amp;lt;- rbinom(n, size=1, prob=0.5) # binary confounder
    W3 &amp;lt;- round(runif(n, min=2, max=7)) # continuous confounder
    W4 &amp;lt;- round(runif(n, min=0, max=4)) # continuous confounder
    A  &amp;lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders
    Y &amp;lt;- rbinom(n, size=1, prob= plogis(-1 + A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4))) # binary outcome depends on confounders
    return(tibble(Y, W1, W2, W3, W4, A))
}

n &amp;lt;- 1000
dat_obs &amp;lt;- generate_data(n) # generate a data set with n observations

Y &amp;lt;- dat_obs$Y
W_A &amp;lt;- dat_obs %&amp;gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner

### Step 1: Estimate Q
Q &amp;lt;- SuperLearner(Y = Y, # Y is the outcome vector
                  X = W_A, # W_A is the matrix of W1, W2, W3, W4, and A
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS
Q_A &amp;lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received
W_A1 &amp;lt;- W_A %&amp;gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &amp;lt;- as.vector(predict(Q, newdata = W_A1)$pred) # predict on that everyone-exposed data set
W_A0 &amp;lt;- W_A %&amp;gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &amp;lt;- as.vector(predict(Q, newdata = W_A0)$pred)
dat_tmle &amp;lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)

### Step 2: Estimate g and compute H(A,W)
A &amp;lt;- dat_obs$A
W &amp;lt;- dat_obs %&amp;gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4
g &amp;lt;- SuperLearner(Y = A, # outcome is the A (treatment) vector
                  X = W, # W is a matrix of predictors
                  family=binomial(), # treatment is a binomial outcome
                  SL.library=sl_libs) # using same candidate learners; could use different learners

g_w &amp;lt;- as.vector(predict(g)$pred) # Pr(A=1|W)
H_1 &amp;lt;- 1/g_w
H_0 &amp;lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)
dat_tmle &amp;lt;- # add clever covariate data to dat_tmle
  dat_tmle %&amp;gt;%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %&amp;gt;%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0

### Step 3: Estimate fluctuation parameter
glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial) # fixed intercept logistic regression
eps &amp;lt;- coef(glm_fit) # save the only coefficient, called epsilon in TMLE lit

### Step 4: Update Q&amp;#39;s
H_A &amp;lt;- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H_A) # updated expected outcome given treatment actually received
Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1) # updated expected outcome for everyone receiving treatment
Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0) # updated expected outcome for everyone not receiving treatment

### Step 5: Compute ATE
tmle_ate &amp;lt;- mean(Q_1_update - Q_0_update) # mean diff in updated expected outcome estimates

### Step 6: compute standard error, CIs and pvals
infl_fn &amp;lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate # influence function
tmle_se &amp;lt;- sqrt(var(infl_fn)/nrow(dat_obs)) # standard error
conf_low &amp;lt;- tmle_ate - 1.96*tmle_se # 95% CI
conf_high &amp;lt;- tmle_ate + 1.96*tmle_se
pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se))) # p-value at alpha .05

tmle_ate
conf_low
conf_high&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.3 (2020-10-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            kableExtra_1.3.1   
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.4.0         tidyr_1.1.2        
## [10] tibble_3.0.4        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.4.2         jsonlite_1.7.2     viridisLite_0.3.0  splines_4.0.3     
##  [5] foreach_1.5.1      modelr_0.1.8       Formula_1.2-4      assertthat_0.2.1  
##  [9] highr_0.8          cellranger_1.1.0   yaml_2.2.1         pillar_1.4.7      
## [13] backports_1.2.1    lattice_0.20-41    glue_1.4.2         digest_0.6.27     
## [17] rvest_0.3.6        colorspace_2.0-0   htmltools_0.5.0    Matrix_1.2-18     
## [21] pkgconfig_2.0.3    broom_0.7.2        earth_5.3.0        haven_2.3.1       
## [25] bookdown_0.21      scales_1.1.1       webshot_0.5.2      ranger_0.12.1     
## [29] TeachingDemos_2.12 generics_0.1.0     ellipsis_0.3.1     withr_2.3.0       
## [33] ROCR_1.0-11        cli_2.2.0          survival_3.2-7     magrittr_2.0.1    
## [37] crayon_1.3.4       readxl_1.3.1       evaluate_0.14      fs_1.5.0          
## [41] fansi_0.4.1        xml2_1.3.2         cabinets_0.6.0     blogdown_0.21     
## [45] tools_4.0.3        hms_0.5.3          lifecycle_0.2.0    munsell_0.5.0     
## [49] reprex_0.3.0       glmnet_4.0-2       plotrix_3.7-8      compiler_4.0.3    
## [53] rlang_0.4.9        plotmo_3.6.0       grid_4.0.3         iterators_1.0.13  
## [57] rstudioapi_0.13    rmarkdown_2.5      gtable_0.3.0       codetools_0.2-16  
## [61] DBI_1.1.0          R6_2.5.0           lubridate_1.7.9.2  knitr_1.30        
## [65] shape_1.4.5        stringi_1.5.3      tmle_1.5.0-1       Rcpp_1.0.5        
## [69] vctrs_0.3.5        dbplyr_2.0.0       tidyselect_1.1.0   xfun_0.19&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Illustrated Guide to TMLE, Part I: Introduction and Motivation</title>
      <link>https://khstats.com/blog/tmle/tutorial/</link>
      <pubDate>Fri, 11 Dec 2020 00:13:14 -0500</pubDate>
      <guid>https://khstats.com/blog/tmle/tutorial/</guid>
      <description>
&lt;link href=&#34;https://khstats.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://khstats.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;blockquote&gt;
&lt;p&gt;The &lt;strong&gt;introductory post&lt;/strong&gt; of a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). This section contains a brief overview of the &lt;strong&gt;targeted learning framework&lt;/strong&gt; and motivation for &lt;strong&gt;semiparametric estimation methods for inference&lt;/strong&gt;, including causal inference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;div id=&#34;table-of-contents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Table of Contents&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post series has three parts:&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;part-i-motivation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Part I: Motivation&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#tmle-in-three-sentences&#34;&gt;TMLE in three sentences üéØ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#an-analysts-motivation-for-learning-tmle&#34;&gt;An Analyst‚Äôs Motivation for Learning TMLE üë©üèº‚Äçüíª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#is-tmle-causal-inference&#34;&gt;Is TMLE Causal Inference? ü§î&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;part-ii-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2&#34;&gt;Part II: Algorithm&lt;/a&gt;&lt;/h3&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/#why-the-visual-guide&#34;&gt;Why the Visual Guide? üé®&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/#tmle-step-by-step&#34;&gt;TMLE, Step-by-Step üö∂üèΩ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/#using-the-tmle-package&#34;&gt;Using the &lt;code&gt;tmle&lt;/code&gt; package üì¶&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;part-iii-evaluation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3&#34;&gt;Part III: Evaluation&lt;/a&gt;&lt;/h3&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/#properties-of-tmle&#34;&gt;Properties of TMLE üìà&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/#why-does-tmle-work&#34;&gt;Why does TMLE work? ‚ú®&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/#resources-to-learn-more&#34;&gt;Resources to learn more ü§ì&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tmle-in-three-sentences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TMLE in three sentences üéØ&lt;/h1&gt;
&lt;p&gt;Targeted Maximum Likelihood Estimation (TMLE) is a semiparametric estimation framework to &lt;strong&gt;estimate a statistical quantity of interest&lt;/strong&gt;. TMLE allows the use of &lt;strong&gt;machine learning&lt;/strong&gt; (ML) models which place &lt;strong&gt;minimal assumptions on the distribution of the data&lt;/strong&gt;. Unlike estimates normally obtained from ML, the &lt;strong&gt;final TMLE estimate will still have valid standard errors for statistical inference&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-analysts-motivation-for-learning-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An Analyst‚Äôs Motivation for Learning TMLE üë©üèº‚Äçüíª&lt;/h1&gt;
&lt;p&gt;When I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It went like this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;If the goal is &lt;span style=&#34;color: #3366ff;&#34;&gt;inference&lt;/span&gt; (e.g., an effect size with a confidence interval), use an &lt;span style=&#34;color: #3366ff;&#34;&gt;interpretable, usually parametric, model&lt;/span&gt; and explain what the coefficients and their standard errors mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the goal is &lt;span style=&#34;color: #cc0000;&#34;&gt;prediction&lt;/span&gt;, use &lt;span style=&#34;color: #cc0000;&#34;&gt;data-adaptive machine learning algorithms&lt;/span&gt; and then look at performance metrics, with the understanding that standard errors, and sometimes even coefficients, no longer exist.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This mentality changed drastically when I started learning about semiparametric estimation methods like TMLE in the context of causal inference. I quickly realized two flaws in this mental framework.&lt;/p&gt;
&lt;p&gt;First, I was thinking about inference backwards: I was choosing a model based on my outcome type (binary, continuous, time-to-event, repeated measures) and then interpreting specific coefficients as my estimates of interest. Yet it makes way more sense to &lt;em&gt;first&lt;/em&gt; determine the statistical quantity, or &lt;strong&gt;estimand&lt;/strong&gt;, that best answers a scientific question, and &lt;em&gt;then&lt;/em&gt; use the method, or &lt;strong&gt;estimator&lt;/strong&gt;, best suited for estimating it. This is the paradigm TMLE is based upon: &lt;strong&gt;we want to build an algorithm, or estimator, targeted to an estimand of interest&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/estimator.png&#34;
    width= 90%
         alt=&#34;Estimator and Estimand&#34;&gt;
&lt;figcaption&gt;
&lt;em&gt;An estimand is a quantity that answers a scientific question of interest. Once we figure out the estimand, we can build an estimator, or algorithm, to estimate it. Image courtesy of Dr.¬†Laura Hatfield and &lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34;&gt;diff.healthpolicydatascience.org&lt;/a&gt;.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Second, I thought flexible, data-adaptive models we commonly classify as statistical and/or &lt;strong&gt;machine learning&lt;/strong&gt; (e.g.¬†LASSO, random forests, gradient boosting, etc.) could only be used for prediction, since they don‚Äôt have &lt;strong&gt;asymptotic properties for inference&lt;/strong&gt; (i.e.¬†standard errors). However, certain &lt;strong&gt;semiparametric estimation methods&lt;/strong&gt; like TMLE can actually use these models to &lt;strong&gt;obtain a final estimate that is closer to the target quantity&lt;/strong&gt; than would be obtained using classic parametric models (e.g.¬†linear and logistic regression). This is because machine learning models are generally designed to accommodate &lt;strong&gt;large numbers of covariates&lt;/strong&gt; with &lt;strong&gt;complex, non-linear relationships&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/parametric_assumptions_comic.jpg&#34; width=100%&gt;
&lt;figcaption&gt;
&lt;em&gt;Semiparametric estimation methods like TMLE can rely on machine learning to avoid making unrealistic parametric assumptions about the underlying distribution of the data (e.g.¬†multivariate normality).&lt;/em&gt;
&lt;/figcaption&gt;
&lt;p&gt;The way we use the machine learning estimates in TMLE, surprisingly enough, yields &lt;strong&gt;known asymptotic properties of bias and variance&lt;/strong&gt; ‚Äì just like we see in parametric maximum likelihood estimation ‚Äì for our target estimand.&lt;/p&gt;
&lt;p&gt;Besides allowing us to compute 95% confidence intervals and p-values for our estimates even after using flexible models, TMLE achieves other beneficial statistical properties, such as &lt;strong&gt;double robustness&lt;/strong&gt;. These are discussed further in &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/&#34;&gt;&lt;em&gt;Part III&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-tmle-causal-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Is TMLE Causal Inference? ü§î&lt;/h1&gt;
&lt;p&gt;If you‚Äôve heard about TMLE before, it was likely in the context of &lt;strong&gt;causal inference&lt;/strong&gt;. Although TMLE was developed for causal inference due to its many attractive properties, it cannot be considered causal inference by itself. Causal inference is a two-step process that first requires &lt;strong&gt;causal assumptions&lt;/strong&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; before a statistical estimand can be interpreted causally.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TMLE can be used to estimate various statistical estimands&lt;/strong&gt; (odds ratio, risk ratio, mean outcome difference, etc.) &lt;strong&gt;even when causal assumptions are not met&lt;/strong&gt;. TMLE is, as its name implies, simply a tool for estimation.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/&#34;&gt;&lt;em&gt;Part II&lt;/em&gt;&lt;/a&gt;, I‚Äôll walk step-by-step through a basic version of the TMLE algorithm: estimating the mean difference in outcomes, adjusted for confounders, for a binary outcome and binary treatment. If causal assumptions are met, this is called the &lt;strong&gt;Average Treatment Effect (ATE)&lt;/strong&gt;, or the mean difference in outcomes in a world in which everyone had received the treatment compared to a world in which everyone had not.&lt;/p&gt;
&lt;p&gt;‚§¥Ô∏è&lt;a href=&#34;#table-of-contents&#34;&gt;&lt;em&gt;Back to the top&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‚û°Ô∏è&lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt2/&#34;&gt;&lt;em&gt;Continue to Part II: The Algorithm&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;References&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;My primary reference for all three posts is &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; by Mark van der Laan and Sherri Rose. I detail many other resources I‚Äôve used to learn TMLE, semiparametric theory, and causal inference in &lt;a href=&#34;https://khstats.com/blog/tmle/tutorial-pt3/&#34;&gt;&lt;em&gt;Part III&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I won‚Äôt discuss causal assumptions in these posts, but this is referring to fundamental assumptions in causal inference like consistency, exchangeability, and positivity. A primary motivation for using TMLE and other semiparametric estimation methods for causal inference is that if you‚Äôve already taken the time to carefully evaluate &lt;em&gt;causal&lt;/em&gt; assumptions, it does not make sense to then damage an otherwise well-designed analysis by making unrealistic &lt;em&gt;statistical&lt;/em&gt; assumptions.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Condensed Key for A Visual Guide to Targeted Maximum Likelihood Estimation (TMLE)</title>
      <link>https://khstats.com/blog/tmle/visual-key/</link>
      <pubDate>Wed, 09 Jan 2019 21:13:14 -0500</pubDate>
      <guid>https://khstats.com/blog/tmle/visual-key/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;A condensed key for my corresponding &lt;a href=&#34;www.khstats.com/blog/tmle/tutorial&#34;&gt;TMLE tutorial&lt;/a&gt; blog post.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;initial-set-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Initial set up&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/1_data_structure.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;strong&gt;Estimand of interest:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ATE = \Psi = E_W[\mathrm{E}[Y|A=1,\mathbf{W}] - \mathrm{E}[Y|A=0,\mathbf{W}]]\]&lt;/span&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 1: Estimate the Outcome
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;First, estimate the expected value of the outcome using treatment and confounders as predictors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(A,\mathbf{W}) = \mathrm{E}[Y|A,\mathbf{W}]\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/2_outcome_fit.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then use that fit to obtain estimates of the expected outcome under varying three different treatment conditions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. If every observation received the treatment they &lt;em&gt;actually&lt;/em&gt; received.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(A,\mathbf{W}) = \mathrm{\hat{E}}[Y|A,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/3_QA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. If every observation received the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(1,\mathbf{W}) = \mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/4_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. If every observation received the control.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(0,\mathbf{W}) = \mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/5_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 2: Estimate the Probability of Treatment
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;The next step is to estimate the probability of treatment, given confounders.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[g(\mathbf{W}) = \mathrm{Pr}(A=1|\mathbf{W})\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/6_treatment_fit.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then we need to compute three different quantities from this model fit:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. The inverse probability of receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(1,\mathbf{W}) = \frac{1}{g(\mathbf{W})} = \frac{1}{\mathrm{Pr}(A=1|\mathbf{W})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/7_H1.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. The negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(0,\mathbf{W}) = -\frac{1}{1-g(\mathbf{W})}= -\frac{1}{\mathrm{Pr}(A=0|\mathbf{W})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/8_H0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(A,\mathbf{W}) = \frac{\mathrm{I}(A=1)}{\mathrm{Pr}(A=1|\mathbf{W})}-\frac{\mathrm{I}(A=0)}{\mathrm{Pr}(A=0|\mathbf{W})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/9_HA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 3: Estimate the Fluctuation Parameter
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;Estimating equation we need to solve:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[logit(\mathrm{E}[Y|A,\mathbf{W}]) = logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}]) + \epsilon H(A,\mathbf{W})\]&lt;/span&gt;
Two technical points for application: we use &lt;code&gt;qlogis&lt;/code&gt; to transform the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; to the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale. Also, the &lt;code&gt;R&lt;/code&gt; code for a fixed intercept is &lt;code&gt;-1 + offset(fixed_intercept)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/10_logistic_regression.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we need to save the coefficient from that logistic regression, which we will call &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/11_epsilon.png&#34; style=&#34;width:40.0%&#34; /&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 4: Update the Initial Estimates of the Expected Outcome
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;em&gt;Note we can use &lt;span class=&#34;math inline&#34;&gt;\(expit\)&lt;/span&gt; to show the inverse of the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; function, and we will denote updates to the outcome regressions as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}^*\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}]) + \hat{\epsilon}H(A,\mathbf{W}))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/update_qAW.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=1,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]) + \hat{\epsilon}H(A,1))\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/12_update_Q1.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=0,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]) + \hat{\epsilon}H(A,0))\]&lt;/span&gt;
&lt;img src=&#34;https://khstats.com/img/tmle/13_update_Q0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 5: Compute the Statistical Estimand of Interest
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We now have updated expected outcomes estimates, so we can compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{ATE}_{TMLE} = \hat{\Psi}_{TMLE} = \sum_{i=1}^{n}[\hat{E^*}[Y|A=1,\mathbf{W}] - \hat{E^*}[Y|A=0,\mathbf{W}]]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/14_compute_ATE.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 6: Calculate the Standard Errors, Confidence Intervals, and P-values
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;To obtain the standard errors, we first need to compute the &lt;strong&gt;Influence Curve&lt;/strong&gt; (IC). The equation for the IC looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{IC} = (Y-\hat{E^*}[Y|A,\mathbf{W}])H(A,\mathbf{W}) + \hat{E^*}[Y|A=1,\mathbf{W}] - \hat{E^*}[Y|A=0,\mathbf{W}] - \hat{ATE}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Once we have the IC, we can take the square-root of its variance divided by the number of observations to get the standard error of our estimate.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{SE} = \sqrt{\frac{var(\hat{IC})}{N}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/tmle/15_ses.png&#34; style=&#34;width:100.0%&#34; /&gt;
Once we have that standard error, we can easily get the 95% confidence interval and p-value of our estimate.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;A visual guide designed as a printable reference is available on my &lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;Github&lt;/a&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://khstats.com/img/TMLE.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
